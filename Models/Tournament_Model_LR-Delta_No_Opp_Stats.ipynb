{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seaborn version:  0.9.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import operator\n",
    "# Non pythonic hack to reuse some utility code\n",
    "if sys.path[0] != '../py_utils':\n",
    "    sys.path.insert(0,'../py_utils')\n",
    "\n",
    "import file_utils    \n",
    "import utils\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from pathlib import Path\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 500)\n",
    "print(\"Seaborn version: \", sns.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missed_predictions(tourney_comp_ratings, numeric_feature_to_scale, prediction_probabilities, X, y, y_pred):\n",
    "    \n",
    "    pred_probs = pd.Series(prediction_probabilities[:,1], index=X.index)\n",
    "    predictions = pd.Series(y_pred, index=y.index)\n",
    "    test_games = tourney_comp_ratings[tourney_comp_ratings.index.isin(X.index)].copy()\n",
    "    test_games[numeric_feature_to_scale] = scaler.inverse_transform(test_games[numeric_feature_to_scale])\n",
    "    test_games['predicted_result'] = predictions\n",
    "    test_games['pred_win_prob'] = pred_probs\n",
    "    missed_predictions = test_games[test_games['game_result'] != \n",
    "                                test_games['predicted_result']].sort_values(by='pred_win_prob', ascending=False)\n",
    "   \n",
    "    missed_predictions.apply(lambda x: feature_dictionary.print_game_info(test_games,x['season_t'], x['round'], x['team_t'] ), axis=1)\n",
    "    supporting_features = missed_predictions.apply(lambda row: utils.get_supporting_features(row,\n",
    "                                                                                         feature_dictionary, \n",
    "                                                                                         feature_list),axis=1)\n",
    "\n",
    "    supporting_model_features = missed_predictions.apply(lambda row: utils.get_supporting_features(row, \n",
    "                                                                                               feature_dictionary,\n",
    "                                                                                               model_features),axis=1)\n",
    "    \n",
    "    missed_predictions = missed_predictions.merge(supporting_features.to_frame(name='supporting_features'),how='left',\n",
    "                                              left_index=True, right_index=True)\n",
    "\n",
    "    missed_predictions = missed_predictions.merge(supporting_model_features.to_frame(name='supporting_model_features'),how='left', \n",
    "                                              left_index=True, right_index=True)\n",
    "\n",
    "    missed_predictions['features'] = 100 * missed_predictions['supporting_features'].apply(lambda x: len(x)) / len(feature_list)\n",
    "\n",
    "    missed_predictions['model_features'] = 100 * missed_predictions['supporting_model_features'].apply(lambda x: len(x)) / \\\n",
    "        len(model_features)\n",
    "\n",
    "    missed_predictions['game_index'] = missed_predictions.index\n",
    "    \n",
    "    return missed_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_features_logistic_regression(classifier, X, y ):\n",
    "    iteration = 0\n",
    "    print(\"Iteration= \", iteration)\n",
    "    iteration += 1\n",
    "    model_stats = {}\n",
    "    drop_list = []\n",
    "    # get baseline by identifying sorted important features using all of the provided features\n",
    "    model_stats = utils.save_model_stats(classifier,X,y,model_stats)\n",
    "    important_features = utils.display_important_features(classifier.coef_[0], X,0)\n",
    "    #important_features = display_important_features_regression(classifier, X,0)\n",
    "    # least important feature\n",
    "    least_important_label = important_features[-1][0]\n",
    "    print(\"least_important label= \", least_important_label)\n",
    "    \n",
    "    drop_list.append(least_important_label)\n",
    "    del important_features[-1]\n",
    "    \n",
    "    # drop list contains all of the feature labels except for the feature label identified as being most important\n",
    "    list_count = len(important_features)\n",
    "    while list_count > 0:\n",
    "        print(\"Iteration= \", iteration)\n",
    "        iteration += 1\n",
    "        model_stats = utils.save_model_stats(classifier,X.drop(columns=drop_list),y,model_stats)\n",
    "        least_important_label = important_features[-1][0]\n",
    "        print(\"least_important label= \", least_important_label)\n",
    "        drop_list.append(least_important_label)\n",
    "        del important_features[-1]\n",
    "        list_count-=1\n",
    "    return model_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_team_file = '../Data/sr_summaries_kaggle_id_no_opp_2018.csv'\n",
    "team_meta_data_file = '../Data/D1_teams.csv'\n",
    "tournament_data_file = '../Data/tournament_results_2018.csv'\n",
    "rankings_data_file = '../data/massey_seasons_with_id.csv'\n",
    "\n",
    "feature_dictionary = utils.Feature_Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# earliest season data starts in 2000 corresponding to tournament season date of 2001\n",
    "start_tournament = 2003\n",
    "stop_tournament = 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in regular season team statistics from SRCBB https://www.sports-reference.com/cbb/\n",
    "\n",
    "#### Read table of team names and associated team meta data from the Kaggle data set.\n",
    "https://console.cloud.google.com/bigquery?project=bigqueryncaa&p=bigquery-public-data&d=ncaa_basketball&page=dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = file_utils.read_summary_team_data(summary_team_file)\n",
    "teams = file_utils.read_team_meta_data(team_meta_data_file)\n",
    "summary_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the NCAA Men's Tournament results from the the Kaggle data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_data = file_utils.read_tournament_results(tournament_data_file,start_tournament)\n",
    "game_data = utils.compute_game_data(tourney_data, teams)\n",
    "computer_rankings = pd.read_csv(Path(rankings_data_file))\n",
    "computer_rankings = computer_rankings[computer_rankings['season'] >= start_tournament]\n",
    "\n",
    "tourney_data = utils.recode_tourney_data(tourney_data)\n",
    "tourney_data = file_utils.merge_tourney_summary_data(tourney_data, summary_data)\n",
    "tourney_data = file_utils.join_tourney_team_data(tourney_data, teams)\n",
    "tourney_comp_ratings = file_utils.merge_tourney_ranking_data(tourney_data, computer_rankings)\n",
    "tourney_comp_ratings = utils.implement_top_conference_feature(tourney_data, teams, game_data, tourney_comp_ratings)\n",
    "tourney_comp_ratings = utils.implement_seed_threshold_feature(tourney_comp_ratings)\n",
    "tourney_comp_ratings = utils.compute_delta_features(tourney_comp_ratings)\n",
    "\n",
    "tourney_comp_ratings.dropna(inplace=True)\n",
    "tourney_comp_ratings[tourney_comp_ratings.isnull().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feature_to_scale = ['delta_margin_victory_avg', 'delta_fg_pct', 'delta_off_rebs_avg',\n",
    "                            'delta_def_rebs_avg', 'delta_ft_pct',\n",
    "                            'delta_to_net_avg', 'delta_win_pct', 'delta_off_rating',\n",
    "                            'delta_ft_att_avg',\n",
    "                            'delta_seed', 'delta_srs', 'delta_sos',\n",
    "                            'delta_sag', 'delta_wlk', 'delta_wol',\n",
    "                            'delta_rth', 'delta_col', 'delta_pom',\n",
    "                            'delta_dol', 'delta_rpi', 'delta_mor']\n",
    "\n",
    "scaler =StandardScaler()\n",
    "tourney_comp_ratings[numeric_feature_to_scale] = scaler.fit_transform(tourney_comp_ratings[numeric_feature_to_scale])\n",
    "tourney_comp_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = tourney_comp_ratings.drop(columns=['round','game_date','seed_t','team_t','team_id_t','team_id_o',\n",
    "                                         'team_o','seed_o','team_id_o','game_result','start_season','game result',\n",
    "                                         'conf_name_t','conf_name_o']).copy()\n",
    "\n",
    "\n",
    "feature_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data.drop(columns=['pts_avg_t','pts_avg_o', 'opp_pts_avg_t','opp_pts_avg_o',\n",
    "                                'margin_victory_avg_t', 'margin_victory_avg_o',\n",
    "                                'poss_avg_t','poss_avg_o',\n",
    "                                'fg_pct_t','fg_pct_o',\n",
    "                                'off_rebs_avg_t','off_rebs_avg_o','def_rebs_avg_t','def_rebs_avg_o',\n",
    "                                'ft_pct_t','ft_pct_o',\n",
    "                                'to_avg_t','to_avg_o','steal_avg_t','steal_avg_o',\n",
    "                                'to_net_avg_t','to_net_avg_o',\n",
    "                                'win_pct_t','win_pct_o','off_rating_t','off_rating_o',\n",
    "                                'ft_att_avg_t','ft_att_avg_o','opp_pts_avg_t','opp_pts_avg_o',\n",
    "                                'srs_t','srs_o','sos_t','sos_o',\n",
    "                                'sag_t','sag_o','wlk_t','wlk_o','wol_t','wol_o',\n",
    "                                'rth_t','rth_o','col_t','col_o','pom_t','pom_o',\n",
    "                                'dol_t','dol_o','rpi_t','rpi_o','mor_t','mor_o'], inplace=True)\n",
    "\n",
    "\n",
    "# for now drop the delta seed features\n",
    "feature_data.drop(columns=['upset_seed_threshold'], inplace=True)\n",
    "feature_data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= feature_data[feature_data['season_t']<= stop_tournament]\n",
    "y=tourney_comp_ratings[tourney_comp_ratings['season_t']<= stop_tournament]['game_result']\n",
    "X= X.drop(columns=['season_t'])\n",
    "\n",
    "feature_list = list(X)\n",
    "feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 5)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "result = logreg.fit(X_train,y_train)\n",
    "\n",
    "print(\"Coeffs \",logreg.coef_)\n",
    "print(\"Intercept \", logreg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.display_important_features(logreg.coef_[0], X_train,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(logreg, \n",
    "                                                        X, \n",
    "                                                        y,\n",
    "                                                        # Number of folds in cross-validation\n",
    "                                                        cv=10,\n",
    "                                                        # Evaluation metric\n",
    "                                                        scoring='accuracy',\n",
    "                                                        # Use all computer cores\n",
    "                                                        n_jobs=-1, \n",
    "                                                        # 50 different sizes of the training set\n",
    "                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n",
    "\n",
    "# Create means and standard deviations of training set scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Create means and standard deviations of test set scores\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Draw lines\n",
    "plt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\n",
    "plt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n",
    "\n",
    "# Draw bands\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n",
    "\n",
    "# Create plot\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.display_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_probabilities = logreg.predict_proba(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"Log loss= \",log_loss(y_test, prediction_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(logreg, X,y, cv=10, scoring='accuracy')\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Feature Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats = eliminate_features_logistic_regression(logreg, X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_accuracy = 0\n",
    "max_cross_val = 0\n",
    "min_log_loss = 10000\n",
    "for key, value in model_stats.items():\n",
    "    accuracy = value['accuracy']\n",
    "    cross_val = value['cross_validation']\n",
    "    log_loss_val = value['log_loss']\n",
    "    print('Accuracy= {0:6.4f} Cross Val= {1:6.4f}  Log Loss= {2:6.4f}'.format(accuracy ,cross_val, log_loss_val ))\n",
    "    if accuracy > max_accuracy:\n",
    "        max_accuracy = accuracy\n",
    "        accuracy_hash = key\n",
    "    if cross_val > max_cross_val:\n",
    "        max_cross_val = cross_val\n",
    "        cross_hash = key\n",
    "    if log_loss_val < min_log_loss:\n",
    "        min_log_loss = log_loss_val\n",
    "        log_hash = key\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print('Max Accuracy= {0:6.4f}'.format( model_stats[accuracy_hash]['accuracy']))\n",
    "print('Max Cross Validation= {0:6.4f}'.format( model_stats[cross_hash]['cross_validation']))\n",
    "print (\"Minimum Log Loss= {0:6.4f}\".format(  model_stats[log_hash]['log_loss']))\n",
    "print('Log Loss at Max Accuracy= {0:6.4f}'.format( model_stats[accuracy_hash]['log_loss'] ))\n",
    "print('Log Loss at Max Cross Validation= {0:6.4f} '.format( model_stats[cross_hash]['log_loss'] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = model_stats[cross_hash]['labels']\n",
    "print(model_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X[model_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 5)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(logreg, \n",
    "                                                        X, \n",
    "                                                        y,\n",
    "                                                        # Number of folds in cross-validation\n",
    "                                                        cv=10,\n",
    "                                                        # Evaluation metric\n",
    "                                                        scoring='accuracy',\n",
    "                                                        # Use all computer cores\n",
    "                                                        n_jobs=-1, \n",
    "                                                        # 50 different sizes of the training set\n",
    "                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n",
    "\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Create means and standard deviations of test set scores\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Draw lines\n",
    "plt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\n",
    "plt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n",
    "\n",
    "# Draw bands\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n",
    "\n",
    "# Create plot\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.display_important_features(logreg.coef_[0], X_train,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "# save model stats\n",
    "prediction_probabilities = logreg.predict_proba(X_test)\n",
    "\n",
    "cross_val_scores = cross_val_score(logreg, X,y, cv=10, scoring='accuracy')\n",
    "cross_validation_average = cross_val_scores.mean()\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"Log loss= \",log_loss(y_test, prediction_probabilities))\n",
    "\n",
    "utils.display_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(logreg, X,y, cv=10, scoring='accuracy')\n",
    "print(\"Cross Validation: \", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_predictions = get_missed_predictions(tourney_comp_ratings, numeric_feature_to_scale, \n",
    "                                            prediction_probabilities, X_test, y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missed_predictions_df = missed_predictions[['game_index','features','model_features']]\n",
    "plot_missed_predictions_df = pd.melt(plot_missed_predictions_df, id_vars='game_index', var_name= 'Features Supporting Outcome')\n",
    "m_plot = sns.barplot(x='game_index', y='value', hue='Features Supporting Outcome', data= plot_missed_predictions_df) \n",
    "plt.title(\"Percentage Of Features Consistent With Game Outcomes\")\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('Missed Prediction Game Index')\n",
    "m_plot.figure.set_size_inches(20,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The bar chart depicts the percentage of features that correctly corresponded to the game outcome but were out weighed by other features in predicting the game incorrectly. Games corresponding to bar heights exceeding 50% should be scrutinized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model Against 2018 Tournament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_year = 2018\n",
    "X_season = feature_data[feature_data['season_t']== test_year][model_features]\n",
    "y_season = tourney_comp_ratings[tourney_comp_ratings['season_t']== test_year]['game_result']\n",
    "X_season.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_season = logreg.predict(X_season)\n",
    "utils.display_confusion_matrix(y_season,y_pred_season)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_probabilities = logreg.predict_proba(X_season)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_season, y_pred_season))\n",
    "print(\"Precision:\", metrics.precision_score(y_season,y_pred_season))\n",
    "print(\"Recall:\",metrics.recall_score(y_season, y_pred_season))\n",
    "print(\"Log loss= \",log_loss(y_season, prediction_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_predictions = get_missed_predictions(tourney_comp_ratings, numeric_feature_to_scale, \n",
    "                                            prediction_probabilities,X_season,y_season,y_pred_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missed_predictions_df = missed_predictions[['game_index','features','model_features']]\n",
    "plot_missed_predictions_df = pd.melt(plot_missed_predictions_df, id_vars='game_index', var_name= 'Features Supporting Outcome')\n",
    "m_plot = sns.barplot(x='game_index', y='value', hue='Features Supporting Outcome', data= plot_missed_predictions_df) \n",
    "plt.title(\"Percentage Of Features Consistent With Game Outcomes\")\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('MIssed Prediction Game Index')\n",
    "m_plot.figure.set_size_inches(20,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
