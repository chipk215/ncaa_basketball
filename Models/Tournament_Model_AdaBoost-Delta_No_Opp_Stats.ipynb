{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seaborn version:  0.9.0\n",
      "sklearn version:  0.20.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import operator\n",
    "import sys\n",
    "import graphviz \n",
    "# Non pythonic hack to reuse some utility code\n",
    "if sys.path[0] != '../py_utils':\n",
    "    sys.path.insert(0,'../py_utils')\n",
    "    \n",
    "import file_utils\n",
    "import utils\n",
    "import sklearn\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import time\n",
    "from sklearn import tree\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 500)\n",
    "print(\"Seaborn version: \", sns.__version__)\n",
    "print(\"sklearn version: \", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_team_file = '../Data/sr_summaries_kaggle_id_no_opp_2018.csv'\n",
    "team_meta_data_file = '../Data/D1_teams.csv'\n",
    "tournament_data_file = '../Data/tournament_results_2018.csv'\n",
    "rankings_data_file = '../data/massey_seasons_with_id.csv'\n",
    "\n",
    "feature_dictionary = utils.Feature_Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not use 2017-2018  for training, we'll hold that data back for testing a season in isolation\n",
    "\n",
    "# These dates correspond to the year in which the tournament was played.\n",
    "# For a start tournament date of 2003, the corresponding season is 2002-2003\n",
    "start_tournament = 2003\n",
    "stop_tournament = 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in regular season team statistics from SRCBB https://www.sports-reference.com/cbb/\n",
    "\n",
    "#### Read table of team names and associated team meta data from the Kaggle data set.\n",
    "\n",
    "https://console.cloud.google.com/bigquery?project=bigqueryncaa&p=bigquery-public-data&d=ncaa_basketball&page=dataset\n",
    "\n",
    "#### Read in the NCAA Men's Tournament results from the the Kaggle data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = file_utils.read_summary_team_data(summary_team_file)\n",
    "teams = file_utils.read_team_meta_data(team_meta_data_file)\n",
    "summary_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_data = file_utils.read_tournament_results(tournament_data_file,start_tournament)\n",
    "game_data = utils.compute_game_data(tourney_data, teams)\n",
    "computer_rankings = pd.read_csv(Path(rankings_data_file))\n",
    "computer_rankings = computer_rankings[computer_rankings['season'] >= start_tournament]\n",
    "\n",
    "tourney_data = utils.recode_tourney_data(tourney_data)\n",
    "tourney_data = file_utils.merge_tourney_summary_data(tourney_data, summary_data)\n",
    "tourney_data = file_utils.join_tourney_team_data(tourney_data, teams)\n",
    "tourney_comp_ratings = file_utils.merge_tourney_ranking_data(tourney_data, computer_rankings)\n",
    "tourney_comp_ratings = utils.implement_top_conference_feature(game_data, tourney_comp_ratings)\n",
    "tourney_comp_ratings = utils.implement_seed_threshold_feature(tourney_comp_ratings)\n",
    "tourney_comp_ratings = utils.compute_delta_features(tourney_comp_ratings)\n",
    "\n",
    "tourney_comp_ratings.dropna(inplace=True)\n",
    "tourney_comp_ratings[tourney_comp_ratings.isnull().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['delta_margin_victory_avg', 'delta_fg_pct', 'delta_off_rebs_avg',\n",
    "                            'delta_def_rebs_avg', 'delta_ft_pct',\n",
    "                            'delta_to_net_avg', 'delta_win_pct', 'delta_off_rating',\n",
    "                            'delta_ft_att_avg',\n",
    "                            'delta_seed', 'delta_srs', 'delta_sos',\n",
    "                            'delta_sag', 'delta_wlk', 'delta_wol',\n",
    "                            'delta_rth', 'delta_col', 'delta_pom',\n",
    "                            'delta_dol', 'delta_mor'] + ['season_t', 'top_conf_t', 'top_conf_o']\n",
    "\n",
    "feature_data = tourney_comp_ratings[feature_columns].copy()\n",
    "feature_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X= feature_data[feature_data['season_t']<= stop_tournament]\n",
    "y=tourney_comp_ratings[tourney_comp_ratings['season_t']<= stop_tournament]['game_result']\n",
    "X= X.drop(columns=['season_t'])\n",
    "\n",
    "feature_list = list(X)\n",
    "feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Train Validate Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_validate, X_test, y_train_validate, y_test = train_test_split(X, y, test_size=0.2, random_state= 5)\n",
    "X_train_1, X_validate, y_train_1, y_validate = train_test_split(X_train_validate, y_train_validate, test_size=0.25, random_state = 5)\n",
    "print(\"Total games= \", X.shape[0])\n",
    "print(\"X Train_1: \",X_train_1.shape)\n",
    "print(\"y_train_1\", y_train_1.shape)\n",
    "print(\"X Validate: \",X_validate.shape)\n",
    "print(\"y_validate\", y_validate.shape)\n",
    "print(\"X Test: \",X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_estimators = 201\n",
    "algorithm_choice = \"SAMME.R\"\n",
    "ada= AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), algorithm=algorithm_choice, n_estimators=number_estimators)\n",
    "    \n",
    "ada.fit(X_train_1, y_train_1)\n",
    "score = ada.score(X_train_1, y_train_1)\n",
    "print(\"Training Model Score= \", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid ={\n",
    "    \"n_estimators\": [51,101,201,301,401,501,601],\n",
    "    \"learning_rate\": [0.01, .05, .1, .5, 1,5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV( AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),algorithm=algorithm_choice), \n",
    "                           param_grid=param_grid, cv=5)\n",
    "\n",
    "start = time()\n",
    "grid_search.fit(X_validate, y_validate)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.cv_results_['params'])))\n",
    "report(grid_search.cv_results_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_estimators = 601\n",
    "#number_estimators = 301\n",
    "learning_rate = 0.5\n",
    "#learning_rate = 1.0\n",
    "ada= AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), algorithm=algorithm_choice, n_estimators=number_estimators,\n",
    "                       learning_rate = learning_rate)\n",
    "\n",
    "X_train = pd.concat([X_train_1, X_validate])\n",
    "y_train = pd.concat([y_train_1, y_validate])\n",
    "print(\"X Train: \",X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "ada.fit(X_train, y_train)\n",
    "score = ada.score(X_train, y_train)\n",
    "print(\"Training Model Score= \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "important_features = utils.display_important_features( ada.feature_importances_, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict={}\n",
    "threshold_dict = {}\n",
    "for stub_estimator in ada.estimators_:\n",
    "    stub_tree = stub_estimator.tree_\n",
    "    stub_feature_index = stub_tree.feature[0]\n",
    "    stub_feature = X_train.columns[stub_feature_index]\n",
    "    if stub_feature in feature_dict:\n",
    "        feature_dict[stub_feature] +=1\n",
    "        threshold_dict[stub_feature].append(stub_tree.threshold[0])\n",
    "    else:\n",
    "        feature_dict[stub_feature] = 1\n",
    "        threshold_dict[stub_feature] = []\n",
    "        threshold_dict[stub_feature].append(stub_tree.threshold[0])\n",
    "        \n",
    "\n",
    "feature_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(feature_dict.keys())\n",
    "values = list(feature_dict.values())\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'feature':labels, 'count':values})\n",
    "ax = df.plot.bar(x='feature', y='count', legend=None)\n",
    "ax.set_title('Estimator Feature Count')\n",
    "ax.figure.set_size_inches(20,6)\n",
    "print(\"Delta Win Count= \",df[df['feature']=='delta_win_pct']['count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_win_partitions = threshold_dict['delta_win_pct']\n",
    "delta_win_partitions = [i*100 for i in delta_win_partitions]\n",
    "\n",
    "df_win = pd.DataFrame(columns=['Delta_Win_Pct','Y'])\n",
    "df_win['Delta_Win_Pct'] = delta_win_partitions\n",
    "df_win['Y'] = 1\n",
    "df_win.sort_values(by='Delta_Win_Pct', inplace=True)\n",
    "print(\"Number of unique partitions= \",df_win['Delta_Win_Pct'].unique().shape[0], \" out of \", df_win.shape[0], ' estimators')\n",
    "\n",
    "splot = sns.scatterplot(x='Delta_Win_Pct', y='Y', data=df_win)\n",
    "splot.figure.set_size_inches(20,6)\n",
    "splot.set_title('Partitions of Delta Win Percentage')\n",
    "splot.set_xticks(np.arange(-25,30,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note 75% duplicate delta win percentage estimators for 103  delta win percentage estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(ada, X_train, y_train, cv=10, scoring='accuracy',\n",
    "                                                        n_jobs=-1, \n",
    "                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n",
    "\n",
    "# Create means and standard deviations of training set scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Create means and standard deviations of test set scores\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Draw lines\n",
    "plt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\n",
    "plt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n",
    "\n",
    "# Draw bands\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n",
    "\n",
    "# Create plot\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = ada.predict(X_test)\n",
    "print(\"AdaBoost model accuracy is %2.2f\" % metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utils.display_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_probabilities = ada.predict_proba(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"Log loss= \",log_loss(y_test, prediction_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cross_val_scores = cross_val_score(ada, X,y, cv=10, scoring='accuracy')\n",
    "print(\"Cross Validation average= \",cross_val_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Retrieve the non-normalized game stats\n",
    "prediction_probabilities[:,1]\n",
    "pred_probs = pd.Series(prediction_probabilities[:,1], index=X_test.index)\n",
    "predictions = pd.Series(y_pred, index=y_test.index)\n",
    "test_games = tourney_comp_ratings[tourney_comp_ratings.index.isin(X_test.index)].copy()\n",
    "\n",
    "test_games['predicted_result'] = predictions\n",
    "test_games['pred_win_prob'] = pred_probs\n",
    "#test_games.head()\n",
    "\n",
    "missed_predictions = test_games[test_games['game_result'] != \n",
    "                                test_games['predicted_result']].sort_values(by='pred_win_prob', ascending=False)\n",
    "\n",
    "print(\"Missed predictions= \", missed_predictions.shape[0])\n",
    "missed_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_dictionary = utils.Feature_Dictionary()\n",
    "missed_predictions.apply(lambda x: feature_dictionary.print_game_info(test_games, \n",
    "                                                                      x['season_t'], x['round'], x['team_t'] ), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "supporting_features = missed_predictions.apply(lambda row: utils.get_supporting_features(row, feature_dictionary, feature_list),\n",
    "                                               axis=1)\n",
    "\n",
    "missed_predictions = missed_predictions.merge(supporting_features.to_frame(name='supporting_features'),how='left',\n",
    "                                              left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "missed_predictions['features'] = 100 * missed_predictions['supporting_features'].apply(lambda x: len(x)) / len(feature_list)\n",
    "\n",
    "missed_predictions['game_index'] = missed_predictions.index\n",
    "#missed_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_missed_predictions_df = missed_predictions[['game_index','features']]\n",
    "plot_missed_predictions_df = pd.melt(plot_missed_predictions_df, id_vars='game_index', var_name= 'Features Supporting Outcome')\n",
    "#plot_missed_predictions_df.head()\n",
    "m_plot = sns.barplot(x='game_index', y='value', hue='Features Supporting Outcome', data= plot_missed_predictions_df) \n",
    "plt.title(\"Percentage Of Features Consistent With Incorrectly Predicted Game Outcomes\")\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('Game Index')\n",
    "m_plot.figure.set_size_inches(20,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The bar chart depicts the percentage of features that correctly corresponded to the game outcome but were out weighed by other features in predicting the game incorrectly. Games corresponding to bar heights exceeding 50% should be scrutinized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missed_predictions_df[plot_missed_predictions_df['value'] > 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examine a missed prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missed_prediction_index = 334\n",
    "# get the list of features that suported the correct game oputcome\n",
    "list_features =missed_predictions.loc[missed_prediction_index]['supporting_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in list_features:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_game_record = missed_predictions.loc[missed_prediction_index]\n",
    "missed_game_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# staged_predict\n",
    "missed_game = X.loc[missed_prediction_index].to_frame().T\n",
    "missed_game\n",
    "staged_predictions = ada.staged_predict(missed_game)\n",
    "item_count =0\n",
    "estimators=[]\n",
    "values = []\n",
    "for item in staged_predictions:\n",
    "    # print(item_count, item)\n",
    "    estimators.append(item_count)\n",
    "    values.append(item[0])\n",
    "    item_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict({'Estimator':estimators,'Value':values})\n",
    "\n",
    "decision_func = []\n",
    "for item in ada.staged_decision_function(missed_game):\n",
    "    decision_func.append(item[0])\n",
    "    #print(item)\n",
    "df['Decision'] = decision_func\n",
    "\n",
    "predict_prob = []\n",
    "for item in ada.staged_predict_proba(missed_game):\n",
    "    predict_prob.append(item[0][0])\n",
    "df['Predict_Prob'] = predict_prob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot('Estimator', 'Value', data= df, marker='', color='blue', label = 'Estimator')\n",
    "plt.plot('Estimator', 'Decision', data= df, marker='', color='black', linestyle='dashed', label='Decision')\n",
    "plt.plot('Estimator', 'Predict_Prob', data= df, marker='', color='red', linestyle='dashed', label='Probability')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# staged_score\n",
    "staged_score_generator = ada.staged_score(missed_game,[1])\n",
    "count =0\n",
    "for item in staged_score_generator:\n",
    "    print(count, item)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ada.estimator_errors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_stubs = ada.estimators_\n",
    "stub_graph_data = []\n",
    "for stub in tree_stubs:\n",
    "    dot_data = tree.export_graphviz(stub,out_file=None, \n",
    "                                feature_names= list(X),filled=True, rounded=True, special_characters=True, proportion=False)\n",
    "    stub_graph_data.append(dot_data)\n",
    "\n",
    "graphviz.Source(stub_graph_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphviz.Source(stub_graph_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphviz.Source(stub_graph_data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphviz.Source(stub_graph_data[332])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stub_graph_data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ada.estimators_[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tree._tree.Tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stub_5 =  ada.estimators_[5]\n",
    "stub_5.classes_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stub_5.tree_.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stub_5.tree_.weighted_n_node_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stub_5.tree_.n_node_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stub_5.tree_.impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2018 Tournament Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_year = 2018\n",
    "X_season = feature_data[feature_data['season_t']== test_year]\n",
    "y_season = tourney_comp_ratings[tourney_comp_ratings['season_t']== test_year]['game_result']\n",
    "X_season= X_season.drop(columns=['season_t'])\n",
    "X_season.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_season = ada.predict(X_season)\n",
    "utils.display_confusion_matrix(y_season,y_pred_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_probabilities =  ada.predict_proba(X_season)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_season, y_pred_season))\n",
    "print(\"Precision:\", metrics.precision_score(y_season,y_pred_season))\n",
    "print(\"Recall:\",metrics.recall_score(y_season, y_pred_season))\n",
    "print(\"Log loss= \",log_loss(y_season, prediction_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
