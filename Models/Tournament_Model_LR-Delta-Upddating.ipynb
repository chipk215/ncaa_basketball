{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import operator\n",
    "# Non pythonic hack to reuse some utility code\n",
    "if sys.path[0] != '../py_utils':\n",
    "    sys.path.insert(0,'../py_utils')\n",
    "\n",
    "import utils\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from pathlib import Path\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 500)\n",
    "print(\"Seaborn version: \", sns.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_features_logistic_regression(classifier, X, y ):\n",
    "    iteration = 0\n",
    "    print(\"Iteration= \", iteration)\n",
    "    iteration += 1\n",
    "    model_stats = {}\n",
    "    drop_list = []\n",
    "    # get baseline by identifying sorted important features using all of the provided features\n",
    "    model_stats = utils.save_model_stats(classifier,X,y,model_stats)\n",
    "    important_features = utils.display_important_features(classifier.coef_[0], X,0)\n",
    "    #important_features = display_important_features_regression(classifier, X,0)\n",
    "    # least important feature\n",
    "    least_important_label = important_features[-1][0]\n",
    "    print(\"least_important label= \", least_important_label)\n",
    "    \n",
    "    drop_list.append(least_important_label)\n",
    "    del important_features[-1]\n",
    "    \n",
    "    # drop list contains all of the feature labels except for the feature label identified as being most important\n",
    "    list_count = len(important_features)\n",
    "    while list_count > 0:\n",
    "        print(\"Iteration= \", iteration)\n",
    "        iteration += 1\n",
    "        model_stats = utils.save_model_stats(classifier,X.drop(columns=drop_list),y,model_stats)\n",
    "        least_important_label = important_features[-1][0]\n",
    "        print(\"least_important label= \", least_important_label)\n",
    "        drop_list.append(least_important_label)\n",
    "        del important_features[-1]\n",
    "        list_count-=1\n",
    "    return model_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in regular season team statistics from SRCBB https://www.sports-reference.com/cbb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = pd.read_csv(Path( '../Data/sr_summaries_kaggle_id.csv'))\n",
    "print(summary_data.shape)\n",
    "\n",
    "#display rows with NaNs\n",
    "summary_data[summary_data.isnull().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop records with NaNs\n",
    "summary_data.dropna(inplace=True)\n",
    "print(summary_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data.rename(str.lower, axis='columns', inplace=True)\n",
    "summary_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read table of team names and associated team meta data from the Kaggle data set.\n",
    "https://console.cloud.google.com/bigquery?project=bigqueryncaa&p=bigquery-public-data&d=ncaa_basketball&page=dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read table of team names and associated team meta data\n",
    "teams = pd.read_csv(Path('../Data/D1_teams.csv'))\n",
    "teams.drop(columns=['code_ncaa','school_ncaa','turner_name','league_name','league_alias','conf_alias',\n",
    "                    'conf_id','division_name','division_alias','division_id',\n",
    "                    'kaggle_team_id','venue_id'], inplace=True)\n",
    "teams.info()\n",
    "teams.set_index('id',inplace=True)\n",
    "teams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the NCAA Men's Tournament results from the the Kaggle data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_data = pd.read_csv(Path('../Data/tournament_results.csv'))\n",
    "tourney_data.drop(columns=['days_from_epoch','day','num_ot','academic_year','win_region','win_alias','lose_region',\n",
    "                           'lose_alias','lose_code_ncaa','win_school_ncaa','win_code_ncaa','win_name','lose_name',\n",
    "                           'win_pts','win_kaggle_team_id','lose_school_ncaa','lose_kaggle_team_id','lose_pts'],inplace=True)\n",
    "\n",
    "tourney_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_data = tourney_data.join(teams, on='win_team_id',how='left')\n",
    "game_data.rename(columns={'kaggle_team_id':'win_kaggle_team_id','conf_name':'win_conf_name'}, inplace=True)\n",
    "game_data = game_data.join(teams,on='lose_team_id',how='left')\n",
    "game_data.rename(columns={'kaggle_team_id':'lose_kaggle_team_id','conf_name':'lose_conf_name'}, inplace=True)\n",
    "games_won_conf = game_data.groupby('win_conf_name').size().reset_index(name='count').sort_values(by=['count'], ascending=False)\n",
    "\n",
    "games_won_conf['percent'] = 100 * games_won_conf['count']/games_won_conf['count'].sum()\n",
    "games_won_conf['cum_percent'] = games_won_conf['percent'].cumsum()\n",
    "games_won_conf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the conferences that have won 70% of all conference games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tournament_conferences_list = games_won_conf[games_won_conf['cum_percent']<= 70]['win_conf_name'].tolist()\n",
    "top_tournament_conferences_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary season data is available from 2010 through 2017.\n",
    "\n",
    "So the intersection of season summary data with tournament data are seasons 2010-2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_data = tourney_data[tourney_data['season'] >= 2010]\n",
    "tourney_data.describe()['season']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode the tourney data so that the teams are not marked with  win/lose status and the game result is encoded as a binary\n",
    "tourney_data['game_result'] = 1\n",
    "tourney_data.game_result = tourney_data.game_result.astype(int)\n",
    "tourney_data.rename(columns={\"win_seed\":\"team_seed\",\"win_market\":\"team\",\"win_team_id\":\"team_id\"}, inplace=True)\n",
    "tourney_data.rename(columns={\"lose_seed\":\"opp_team_seed\",\"lose_market\":\"opp_team\",\"lose_team_id\":\"opp_team_id\"}, inplace=True)\n",
    "tourney_data['start_season'] = tourney_data['season'] -1\n",
    "\n",
    "#create some temporary buffer columns\n",
    "tourney_data['copy_team'] = tourney_data['team']\n",
    "tourney_data['copy_team_seed'] = tourney_data['team_seed']\n",
    "tourney_data['copy_team_id'] = tourney_data['team_id']\n",
    "\n",
    "#swap the team and opp team data\n",
    "tourney_data.loc[1::2,'team'] = tourney_data.loc[1::2,'opp_team']\n",
    "tourney_data.loc[1::2,'opp_team'] = tourney_data.loc[1::2,'copy_team']\n",
    "tourney_data.loc[1::2,'team_seed'] = tourney_data.loc[1::2,'opp_team_seed']\n",
    "tourney_data.loc[1::2,'opp_team_seed'] = tourney_data.loc[1::2,'copy_team_seed']\n",
    "tourney_data.loc[1::2,'team_id'] = tourney_data.loc[1::2,'opp_team_id']\n",
    "tourney_data.loc[1::2,'opp_team_id'] = tourney_data.loc[1::2,'copy_team_id']\n",
    "\n",
    "# flip the game result\n",
    "tourney_data.loc[1::2,'game_result'] = 0\n",
    "\n",
    "#drop the temporary columns\n",
    "tourney_data.drop(columns=['copy_team','copy_team_seed','copy_team_id'],inplace=True)\n",
    "tourney_data.rename(columns={\"team_seed\":\"seed_t\",\"opp_team_seed\":\"seed_o\"}, inplace=True)\n",
    "\n",
    "tourney_data['Game Result'] = tourney_data.game_result.map({1:'Win', 0:'Lose'})\n",
    "tourney_data = tourney_data.merge(summary_data, left_on=['start_season','team_id'], \n",
    "                                right_on=['season', 'team_id'],how='left',suffixes=('','_y'))\n",
    "\n",
    "tourney_data.drop(columns=['season_y'],inplace=True)\n",
    "tourney_data = tourney_data.merge(summary_data, left_on=['start_season','opp_team_id'], \n",
    "                                right_on=['season', 'team_id'], how='left',suffixes=('_t','_o'))\n",
    "\n",
    "tourney_data.drop(columns=['school_t','school_o','games_t','games_o','team_id_o'],inplace=True)\n",
    "\n",
    "tourney_data = tourney_data.join(teams, on='team_id_t', how='left')\n",
    "tourney_data = tourney_data.join(teams, on='opp_team_id', how='left', lsuffix='_t', rsuffix='_o')\n",
    "tourney_data.rename(index=str,columns={'team':'team_t','opp_team':'team_o', 'opp_team_id':'team_id_o'},inplace=True)\n",
    "\n",
    "tourney_data['game_result'] = tourney_data.game_result.apply(utils.negate_loser)\n",
    "tourney_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computer_rankings = pd.read_csv(Path( '../Data/massey_seasons_with_id.csv'))\n",
    "computer_rankings = computer_rankings[computer_rankings['season']>=2010]\n",
    "computer_rankings[computer_rankings.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_merge = tourney_data.merge(computer_rankings, left_on=['season_t','team_id_t'], \n",
    "                                right_on=['season', 'kaggle_id'],how='left',suffixes=('','_y'))\n",
    "temp_merge.drop(columns=['Team','season','win_pct','kaggle_id'],inplace=True)\n",
    "temp_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_comp_ratings = temp_merge.merge(computer_rankings,left_on=['season_t','team_id_o'], \n",
    "                                right_on=['season', 'kaggle_id'], how='left',suffixes=('_t','_o'))\n",
    "\n",
    "tourney_comp_ratings.drop(columns=['Team','season','win_pct','kaggle_id'],inplace=True)\n",
    "\n",
    "tourney_comp_ratings.rename(str.lower, axis='columns', inplace=True)\n",
    "tourney_comp_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a feature\n",
    "top_conf = 1 if team is a top conference and opponent is not\n",
    "top_conf = -1 if opp_team is a top_conference and team is not\n",
    "top_conf = 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_tournament_conferences_list)\n",
    "tourney_comp_ratings['top_conf'] = tourney_comp_ratings.apply(lambda row: utils.conf_compare(row.conf_name_t, \n",
    "                                                                                             row.conf_name_o,\n",
    "                                                                                             top_tournament_conferences_list),\n",
    "                                                              axis=1)\n",
    "\n",
    "tourney_comp_ratings.head(10)[['game_result','conf_name_t','conf_name_o','top_conf']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsets occur less than 30% of the time when seeding deltas are more than 6\n",
    "tourney_comp_ratings['upset_seed_threshold'] = tourney_comp_ratings.apply(\n",
    "    lambda row: abs(row.seed_t - row.seed_o) > 6, axis=1).astype(int)\n",
    "    \n",
    "tourney_comp_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_comp_ratings['margin_victory_avg_t'] = tourney_comp_ratings['pts_avg_t'] - tourney_comp_ratings['opp_pts_avg_t']\n",
    "tourney_comp_ratings['margin_victory_avg_o'] = tourney_comp_ratings['pts_avg_o'] - tourney_comp_ratings['opp_pts_avg_o']\n",
    "\n",
    "tourney_comp_ratings['delta_margin_victory_avg'] = tourney_comp_ratings['margin_victory_avg_t'] - \\\n",
    "    tourney_comp_ratings['margin_victory_avg_o']\n",
    "\n",
    "tourney_comp_ratings['delta_fg_pct'] = tourney_comp_ratings['fg_pct_t'] - tourney_comp_ratings['fg_pct_o']\n",
    "\n",
    "#tourney_comp_ratings['allow_fg_pct'] = tourney_comp_ratings['allow_fg_pct_t'] - tourney_comp_ratings['allow_fg_pct_o']\n",
    "tourney_comp_ratings['delta_off_rebs_avg'] = tourney_comp_ratings['off_rebs_avg_t'] - tourney_comp_ratings['off_rebs_avg_o']\n",
    "\n",
    "tourney_comp_ratings['delta_def_rebs_avg'] = tourney_comp_ratings['def_rebs_avg_t'] - tourney_comp_ratings['def_rebs_avg_o']\n",
    "\n",
    "tourney_comp_ratings['delta_allow_def_rebs_avg'] = tourney_comp_ratings['allow_def_rebs_avg_t'] - \\\n",
    "    tourney_comp_ratings['allow_def_rebs_avg_o']\n",
    "\n",
    "tourney_comp_ratings['delta_ft_pct'] = tourney_comp_ratings['ft_pct_t'] - tourney_comp_ratings['ft_pct_o']\n",
    "\n",
    "tourney_comp_ratings['to_net_avg_t'] = tourney_comp_ratings['to_avg_t'] - tourney_comp_ratings['steal_avg_t']\n",
    "\n",
    "tourney_comp_ratings['to_net_avg_o'] = tourney_comp_ratings['to_avg_o'] - tourney_comp_ratings['steal_avg_o']\n",
    "\n",
    "tourney_comp_ratings['delta_to_net_avg'] = tourney_comp_ratings['to_net_avg_t'] - tourney_comp_ratings['to_net_avg_o']\n",
    "\n",
    "tourney_comp_ratings['delta_win_pct'] = tourney_comp_ratings['win_pct_t'] - tourney_comp_ratings['win_pct_o']\n",
    "\n",
    "tourney_comp_ratings['delta_off_rating'] = tourney_comp_ratings['off_rating_t'] - tourney_comp_ratings['off_rating_o']\n",
    "\n",
    "tourney_comp_ratings['delta_allow_off_rebs_avg'] = tourney_comp_ratings['allow_off_rebs_avg_t'] - \\\n",
    "    tourney_comp_ratings['allow_off_rebs_avg_o']\n",
    "\n",
    "tourney_comp_ratings['delta_ft_att_avg'] = tourney_comp_ratings['ft_att_avg_t'] - tourney_comp_ratings['ft_att_avg_o']\n",
    "\n",
    "tourney_comp_ratings['delta_allow_ft_att_avg'] = tourney_comp_ratings['allow_ft_att_avg_t'] -  \\\n",
    "    tourney_comp_ratings['allow_ft_att_avg_o']\n",
    "\n",
    "tourney_comp_ratings['delta_seed'] = tourney_comp_ratings['seed_t'] - tourney_comp_ratings['seed_o']\n",
    "\n",
    "tourney_comp_ratings['delta_srs'] = tourney_comp_ratings['srs_t'] - tourney_comp_ratings['srs_o']\n",
    "tourney_comp_ratings['delta_sos'] = tourney_comp_ratings['sos_t'] - tourney_comp_ratings['sos_o']\n",
    "\n",
    "tourney_comp_ratings['delta_sag'] = tourney_comp_ratings['sag_t'] - tourney_comp_ratings['sag_o']\n",
    "tourney_comp_ratings['delta_wlk'] = tourney_comp_ratings['wlk_t'] - tourney_comp_ratings['wlk_o']\n",
    "tourney_comp_ratings['delta_wol'] = tourney_comp_ratings['wol_t'] - tourney_comp_ratings['wol_o']\n",
    "tourney_comp_ratings['delta_rth'] = tourney_comp_ratings['rth_t'] - tourney_comp_ratings['rth_o']\n",
    "tourney_comp_ratings['delta_col'] = tourney_comp_ratings['col_t'] - tourney_comp_ratings['col_o']\n",
    "tourney_comp_ratings['delta_pom'] = tourney_comp_ratings['pom_t'] - tourney_comp_ratings['pom_o']\n",
    "tourney_comp_ratings['delta_dol'] = tourney_comp_ratings['dol_t'] - tourney_comp_ratings['dol_o']\n",
    "tourney_comp_ratings['delta_rpi'] = tourney_comp_ratings['rpi_t'] - tourney_comp_ratings['rpi_o']\n",
    "tourney_comp_ratings['delta_mor'] = tourney_comp_ratings['mor_t'] - tourney_comp_ratings['mor_o']\n",
    "\n",
    "\n",
    "tourney_comp_ratings.drop(columns=['season_o'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feature_to_scale = ['delta_margin_victory_avg', 'delta_fg_pct', 'delta_off_rebs_avg',\n",
    "                            'delta_def_rebs_avg', 'delta_allow_def_rebs_avg', 'delta_ft_pct',\n",
    "                            'delta_to_net_avg', 'delta_win_pct', 'delta_off_rating',\n",
    "                            'delta_allow_off_rebs_avg', 'delta_ft_att_avg', 'delta_allow_ft_att_avg',\n",
    "                            'delta_seed', 'delta_srs', 'delta_sos',\n",
    "                            'delta_sag', 'delta_wlk', 'delta_wol',\n",
    "                            'delta_rth', 'delta_col', 'delta_pom',\n",
    "                            'delta_dol', 'delta_rpi', 'delta_mor']\n",
    "\n",
    "scaler =StandardScaler()\n",
    "tourney_comp_ratings[numeric_feature_to_scale] = scaler.fit_transform(tourney_comp_ratings[numeric_feature_to_scale])\n",
    "tourney_comp_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = tourney_comp_ratings.drop(columns=['round','game_date','seed_t','team_t','team_id_t','team_id_o',\n",
    "                                         'team_o','seed_o','team_id_o','game_result','start_season','game result',\n",
    "                                         'conf_name_t','conf_name_o']).copy()\n",
    "\n",
    "\n",
    "feature_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data.drop(columns=['pts_avg_t','pts_avg_o', 'opp_pts_avg_t','opp_pts_avg_o',\n",
    "                                'margin_victory_avg_t', 'margin_victory_avg_o',\n",
    "                                'poss_avg_t','poss_avg_o',\n",
    "                                'fg_pct_t','fg_pct_o','allow_fg_pct_t','allow_fg_pct_o',\n",
    "                                'off_rebs_avg_t','off_rebs_avg_o','def_rebs_avg_t','def_rebs_avg_o',\n",
    "                                'allow_def_rebs_avg_t','allow_def_rebs_avg_o','ft_pct_t','ft_pct_o',\n",
    "                                'to_avg_t','to_avg_o','steal_avg_t','steal_avg_o',\n",
    "                                'to_net_avg_t','to_net_avg_o',\n",
    "                                'win_pct_t','win_pct_o','off_rating_t','off_rating_o',\n",
    "                                'allow_off_rebs_avg_t','allow_off_rebs_avg_o',\n",
    "                                'ft_att_avg_t','ft_att_avg_o','opp_pts_avg_t','opp_pts_avg_o',\n",
    "                                'srs_t','srs_o','sos_t','sos_o',\n",
    "                                'allow_ft_att_avg_t','allow_ft_att_avg_o',\n",
    "                                'sag_t','sag_o','wlk_t','wlk_o','wol_t','wol_o',\n",
    "                                'rth_t','rth_o','col_t','col_o','pom_t','pom_o',\n",
    "                                'dol_t','dol_o','rpi_t','rpi_o','mor_t','mor_o'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= feature_data[feature_data['season_t']>=2010]\n",
    "y=tourney_comp_ratings[tourney_comp_ratings['season_t']>=2010]['game_result']\n",
    "X= X.drop(columns=['season_t'])\n",
    "\n",
    "feature_list = list(X)\n",
    "feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 5)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark the records used for training\n",
    "#tourney_comp_ratings['train_rec'] = 0\n",
    "#tourney_comp_ratings.loc[X_train.index,'train_rec']= 1\n",
    "tourney_comp_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "\n",
    "result = logreg.fit(X_train,y_train)\n",
    "\n",
    "print(\"Coeffs \",logreg.coef_)\n",
    "print(\"Intercept \", logreg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.display_important_features(logreg.coef_[0], X_train,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.display_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_probabilities = logreg.predict_proba(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"Log loss= \",log_loss(y_test, prediction_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(logreg, X,y, cv=10, scoring='accuracy')\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(logreg, \n",
    "                                                        X, \n",
    "                                                        y,\n",
    "                                                        # Number of folds in cross-validation\n",
    "                                                        cv=10,\n",
    "                                                        # Evaluation metric\n",
    "                                                        scoring='accuracy',\n",
    "                                                        # Use all computer cores\n",
    "                                                        n_jobs=-1, \n",
    "                                                        # 50 different sizes of the training set\n",
    "                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n",
    "\n",
    "# Create means and standard deviations of training set scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Create means and standard deviations of test set scores\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Draw lines\n",
    "plt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\n",
    "plt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n",
    "\n",
    "# Draw bands\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n",
    "\n",
    "# Create plot\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Feature Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feature_data[feature_data['season_t']>=2010]\n",
    "X = X.drop(columns=['season_t'])\n",
    "y = tourney_comp_ratings[tourney_comp_ratings['season_t']>=2010]['game_result']\n",
    "\n",
    "model_stats = eliminate_features_logistic_regression(logreg, X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_accuracy = 0\n",
    "max_cross_val = 0\n",
    "min_log_loss = 10000\n",
    "for key, value in model_stats.items():\n",
    "    accuracy = value['accuracy']\n",
    "    cross_val = value['cross_validation']\n",
    "    log_loss_val = value['log_loss']\n",
    "    print('Accuracy= {0:6.4f} Cross Val= {1:6.4f}  Log Loss= {2:6.4f}'.format(accuracy ,cross_val, log_loss_val ))\n",
    "    if accuracy > max_accuracy:\n",
    "        max_accuracy = accuracy\n",
    "        accuracy_hash = key\n",
    "    if cross_val > max_cross_val:\n",
    "        max_cross_val = cross_val\n",
    "        cross_hash = key\n",
    "    if log_loss_val < min_log_loss:\n",
    "        min_log_loss = log_loss_val\n",
    "        log_hash = key\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print('Max Accuracy= {0:6.4f}'.format( model_stats[accuracy_hash]['accuracy']))\n",
    "print('Max Cross Validation= {0:6.4f}'.format( model_stats[cross_hash]['cross_validation']))\n",
    "print (\"Minimum Log Loss= {0:6.4f}\".format(  model_stats[log_hash]['log_loss']))\n",
    "print('Log Loss at Max Accuracy= {0:6.4f}'.format( model_stats[accuracy_hash]['log_loss'] ))\n",
    "print('Log Loss at Max Cross Validation= {0:6.4f} '.format( model_stats[cross_hash]['log_loss'] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = model_stats[cross_hash]['labels']\n",
    "print(model_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feature_data[feature_data['season_t']>=2010][model_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tourney_comp_ratings[tourney_comp_ratings['season_t']>=2010]['game_result']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 5)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "# save model stats\n",
    "prediction_probabilities = logreg.predict_proba(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "precision = metrics.precision_score(y_test, y_pred)\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "log_loss_value = log_loss(y_test, prediction_probabilities)\n",
    "cross_val_scores = cross_val_score(logreg, X,y, cv=10, scoring='accuracy')\n",
    "cross_validation_average = cross_val_scores.mean()\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(logreg, \n",
    "                                                        X, \n",
    "                                                        y,\n",
    "                                                        # Number of folds in cross-validation\n",
    "                                                        cv=10,\n",
    "                                                        # Evaluation metric\n",
    "                                                        scoring='accuracy',\n",
    "                                                        # Use all computer cores\n",
    "                                                        n_jobs=-1, \n",
    "                                                        # 50 different sizes of the training set\n",
    "                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n",
    "utils.display_confusion_matrix(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.display_important_features(logreg.coef_[0], X_train,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_probabilities = logreg.predict_proba(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"Log loss= \",log_loss(y_test, prediction_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Create means and standard deviations of test set scores\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Draw lines\n",
    "plt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\n",
    "plt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n",
    "\n",
    "# Draw bands\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n",
    "\n",
    "# Create plot\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the non-normalized game stats\n",
    "prediction_probabilities = logreg.predict_proba(X_test)\n",
    "prediction_probabilities[:,1]\n",
    "pred_probs = pd.Series(prediction_probabilities[:,1], index=X_test.index)\n",
    "predictions = pd.Series(y_pred, index=y_test.index)\n",
    "test_games = tourney_comp_ratings[tourney_comp_ratings.index.isin(X_test.index)].copy()\n",
    "\n",
    "test_games[numeric_feature_to_scale] = scaler.inverse_transform(test_games[numeric_feature_to_scale])\n",
    "test_games['predicted_result'] = predictions\n",
    "test_games['pred_win_prob'] = pred_probs\n",
    "\n",
    "test_games.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "missed_predictions = test_games[test_games['game_result'] != \n",
    "                                test_games['predicted_result']].sort_values(by='pred_win_prob', ascending=False)\n",
    "\n",
    "print(\"Missed predictions= \", missed_predictions.shape[0])\n",
    "\n",
    "missed_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dictionary = utils.Feature_Dictionary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_predictions.apply(lambda x: feature_dictionary.print_game_info(test_games,x['season_t'], x['round'], x['team_t'] ), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supporting_features = missed_predictions.apply(lambda row: utils.get_supporting_features(row,\n",
    "                                                                                         feature_dictionary, \n",
    "                                                                                         feature_list),axis=1)\n",
    "\n",
    "supporting_model_features = missed_predictions.apply(lambda row: utils.get_supporting_features(row, \n",
    "                                                                                               feature_dictionary,\n",
    "                                                                                               model_features),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_predictions = missed_predictions.merge(supporting_features.to_frame(name='supporting_features'),how='left',\n",
    "                                              left_index=True, right_index=True)\n",
    "\n",
    "missed_predictions = missed_predictions.merge(supporting_model_features.to_frame(name='supporting_model_features'),how='left', \n",
    "                                              left_index=True, right_index=True)\n",
    "\n",
    "missed_predictions['features'] = 100 * missed_predictions['supporting_features'].apply(lambda x: len(x)) / len(feature_list)\n",
    "\n",
    "missed_predictions['model_features'] = 100 * missed_predictions['supporting_model_features'].apply(lambda x: len(x)) / \\\n",
    "    len(model_features)\n",
    "\n",
    "missed_predictions['game_index'] = missed_predictions.index\n",
    "missed_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missed_predictions_df = missed_predictions[['game_index','features','model_features']]\n",
    "plot_missed_predictions_df = pd.melt(plot_missed_predictions_df, id_vars='game_index', var_name= 'Features Supporting Outcome')\n",
    "plot_missed_predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_plot = sns.barplot(x='game_index', y='value', hue='Features Supporting Outcome', data= plot_missed_predictions_df) \n",
    "plt.title(\"Percentage Of Features Consistent With Incorrectly Predicted Game Outcomes\")\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('Game Index')\n",
    "m_plot.figure.set_size_inches(20,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The bar chart depicts the percentage of features that correctly corresponded to the game outcome but were out weighed by other features in predicting the game incorrectly. Games corresponding to bar heights exceeding 50% should be scrutinized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_predictions[missed_predictions['game_index']==50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supporting_model_list = missed_predictions[missed_predictions['game_index']==50]['supporting_model_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in supporting_model_list:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tourney_comp_ratings[['delta_sos','delta_srs']]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "linregress(temp.delta_sos, temp.delta_srs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
